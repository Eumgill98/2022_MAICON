{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ANJKdai1ymzj","executionInfo":{"status":"error","timestamp":1668307312524,"user_tz":-540,"elapsed":415,"user":{"displayName":"김다흰","userId":"10961180094948908835"}},"outputId":"943b4ee2-968a-4af6-b4c3-3139961d7ceb","colab":{"base_uri":"https://localhost:8080/","height":245}},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-b32d0704abcc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# dataloader : image_generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mimage_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'Dataset' is not defined"]}],"source":["# dataloader : image_generator\n","\n","class image_generator(Dataset):\n","    def __init__(self, paths, input_size, scaler, mode='train', logger=None, verbose=False):\n","        self.x_paths = paths\n","        self.y_paths = list(map(lambda x : x.replace('x', 'y'),self.x_paths))\n","        self.input_size = input_size\n","        self.scaler = scaler\n","        self.logger = logger\n","        self.verbose = verbose\n","        self.mode = mode\n","        \n","    def __len__(self):\n","        return len(self.x_paths)\n","    \n","    def __getitem__(self, id_: int):\n","        filename = os.path.basename(self.x_paths[id_])\n","        image = cv2.imread(self.x_paths[id_], cv2.IMREAD_COLOR)\n","        orig_size = image.shape\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","        half = orig_size[1]//2\n","        img1 = image[:,:half,:]\n","        img2 = image[:,half:,:]\n","\n","        difference = img1 - img2\n","        difference_size = difference.shape\n","\n","        x = cv2.resize(difference, self.input_size)\n","        x = self.scaler(x)\n","        x = np.transpose(x, (2, 0, 1))\n","\n","        if self.mode in ['train', 'valid']:\n","            y = cv2.imread(self.y_paths[id_], cv2.IMREAD_GRAYSCALE)\n","            y_seg = y[:,y.shape[1]//2:]\n","            y_seg = cv2.resize(y_seg, self.input_size, interpolation=cv2.INTER_NEAREST)\n","        \n","            return x, y_seg, filename\n","        \n","        elif self.mode in ['test']:\n","            return x, difference_size, filename\n","\n","        else:\n","            assert False, f\"Invalid mode : {self.mode}\""]},{"cell_type":"code","source":["\"\"\"\n","Predict\n","\"\"\"\n","from datetime import datetime\n","from tqdm import tqdm\n","import numpy as np\n","import random, os, sys, torch, cv2, warnings\n","from glob import glob\n","from torch.utils.data import DataLoader\n","\n","prj_dir = os.path.dirname(os.path.abspath(__file__))\n","sys.path.append(prj_dir)\n","\n","from baseline_modules.utils import load_yaml, save_yaml, get_logger\n","from baseline_modules.scalers import get_image_scaler\n","from baseline_modules.datasets import SegDataset, image_generator\n","from models.utils import get_model\n","warnings.filterwarnings('ignore')\n","\n","if __name__ == '__main__':\n","\n","    #! Load config\n","    config = load_yaml(os.path.join(prj_dir, 'config', 'predict.yaml'))\n","    train_config = load_yaml(os.path.join(prj_dir, 'results', 'train', config['train_serial'], 'train.yaml'))\n","    \n","    #! Set predict serial\n","    pred_serial = config['train_serial'] + '_' + datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","\n","    # Set random seed, deterministic\n","    torch.cuda.manual_seed(train_config['seed'])\n","    torch.manual_seed(train_config['seed'])\n","    np.random.seed(train_config['seed'])\n","    random.seed(train_config['seed'])\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","    # Set device(GPU/CPU)\n","    os.environ['CUDA_VISIBLE_DEVICES'] = str(config['gpu_num'])\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    \n","    # Create train result directory and set logger\n","    pred_result_dir = os.path.join(prj_dir, 'results', 'pred', pred_serial)\n","    pred_result_dir_mask = os.path.join(prj_dir, 'results', 'pred', pred_serial, 'mask')\n","    os.makedirs(pred_result_dir, exist_ok=True)\n","    os.makedirs(pred_result_dir_mask, exist_ok=True)\n","\n","    # Set logger\n","    logging_level = 'debug' if config['verbose'] else 'info'\n","    logger = get_logger(name='train',\n","                        file_path=os.path.join(pred_result_dir, 'pred.log'),\n","                        level=logging_level)\n","\n","    # Set data directory\n","    test_dirs = os.path.join(prj_dir, 'data', 'test')\n","    test_img_paths = glob(os.path.join(test_dirs, 'x', '*.png'))\n","\n","    #! Load data & create dataset for train \n","    test_dataset = image_generator(paths=test_img_paths,\n","                            input_size=[train_config['input_width'], train_config['input_height']],\n","                            scaler=get_image_scaler(train_config['scaler']),\n","                            mode='test',\n","                            logger=logger)\n","\n","    # Create data loader\n","    test_dataloader = DataLoader(dataset=test_dataset,\n","                                batch_size=config['batch_size'],\n","                                num_workers=config['num_workers'],\n","                                shuffle=False,\n","                                drop_last=False)\n","    logger.info(f\"Load test dataset: {len(test_dataset)}\")\n","\n","    # Load architecture\n","    model = get_model(model_str=train_config['architecture'])\n","    model = model(\n","                classes=train_config['n_classes'],\n","                encoder_name=train_config['encoder'],\n","                encoder_weights=train_config['encoder_weight'],\n","                activation=train_config['activation']).to(device)\n","    logger.info(f\"Load model architecture: {train_config['architecture']}\")\n","\n","    #! Load weight\n","    check_point_path = os.path.join(prj_dir, 'results', 'train', config['train_serial'], 'model.pt')\n","    check_point = torch.load(check_point_path)\n","    model.load_state_dict(check_point['model'])\n","    logger.info(f\"Load model weight, {check_point_path}\")\n","\n","    # Save config\n","    save_yaml(os.path.join(pred_result_dir, 'train_config.yml'), train_config)\n","    save_yaml(os.path.join(pred_result_dir, 'predict_config.yml'), config)\n","    \n","    # Predict\n","    logger.info(f\"START PREDICTION\")\n","\n","    model.eval()\n","\n","    with torch.no_grad():\n","\n","        for batch_id, (x, difference_size, filename) in enumerate(tqdm(test_dataloader)):\n","            \n","            x = x.to(device, dtype=torch.float)\n","            y_pred = model(x)\n","            y_pred_argmax = y_pred.argmax(1).cpu().numpy().astype(np.uint8)\n","            difference_size = [(difference_size[0].tolist()[i], difference_size[1].tolist()[i]) for i in range(len(difference_size[0]))]\n","\n","            # Save predict result\n","            for filename_, orig_size_, y_pred_ in zip(filename, difference_size, y_pred_argmax):\n","                resized_img = cv2.resize(y_pred_, [orig_size_[1], orig_size_[0]], interpolation=cv2.INTER_NEAREST)\n","                blank_img = np.zeros([orig_size_[0], orig_size_[1]], dtype=np.uint8)\n","                concated_img = cv2.hconcat([blank_img, resized_img])\n","                cv2.imwrite(os.path.join(pred_result_dir_mask, filename_), concated_img)\n","    \n","    logger.info(f\"END PREDICTION\")"],"metadata":{"id":"lASPIUsyy4zO","executionInfo":{"status":"error","timestamp":1668307309580,"user_tz":-540,"elapsed":5193,"user":{"displayName":"김다흰","userId":"10961180094948908835"}},"outputId":"5775c27f-fbc6-4bcb-9a42-adbbe08a1bb1","colab":{"base_uri":"https://localhost:8080/","height":245}},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-c6aac587ccb9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mprj_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprj_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"]}]}]}